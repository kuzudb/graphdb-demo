{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: GraphQA using LangChain\n",
    "In this notebook, we will use the LangChain library to answer questions on top of the KÃ¹zu graph we\n",
    "just created in the previous section. The example below uses the OpenAI GPT-3.5 turbo model to generate\n",
    "Cypher and answer questions via a text-to-Cypher pipeline, but you can use any other model and see\n",
    "how it performs.\n",
    "\n",
    "We start by opening a connection to the existing database and loading the `OPENAI_API_KEY` variable\n",
    "from a local `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install python-dotenv langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kuzu\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"Please set OPENAI_API_KEY in the .env file\"\n",
    "\n",
    "db = kuzu.Database(\"db\")\n",
    "conn = kuzu.Connection(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import KuzuQAChain\n",
    "from langchain_community.graphs import KuzuGraph\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties: [{'properties': [('country', 'STRING')], 'label': 'Country'}, {'properties': [('customer_id', 'INT64'), ('name', 'STRING'), ('age', 'INT64')], 'label': 'Customer'}, {'properties': [('id', 'INT64'), ('title', 'STRING'), ('country', 'STRING'), ('description', 'STRING'), ('variety', 'STRING'), ('points', 'INT64'), ('price', 'DOUBLE'), ('state', 'STRING'), ('taster_name', 'STRING'), ('taster_twitter_handle', 'STRING')], 'label': 'Wine'}, {'properties': [('taster_twitter_handle', 'STRING'), ('taster_name', 'STRING'), ('taster_id', 'STRING'), ('pagerank', 'DOUBLE')], 'label': 'Taster'}]\n",
      "Relationships properties: [{'properties': [], 'label': 'Tasted'}, {'properties': [], 'label': 'Purchased'}, {'properties': [], 'label': 'LivesIn'}, {'properties': [], 'label': 'Follows'}, {'properties': [], 'label': 'IsFrom'}]\n",
      "Relationships: ['(:Taster)-[:Tasted]->(:Wine)', '(:Customer)-[:Purchased]->(:Wine)', '(:Customer)-[:LivesIn]->(:Country)', '(:Customer)-[:Follows]->(:Taster)', '(:Wine)-[:IsFrom]->(:Country)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a graph object for KuzuQAChain and print the schema\n",
    "graph = KuzuGraph(db)\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema is passed as part of the prompt to the LLM, which is then used to generate the Cypher query.\n",
    "The following example shows how to the GPT-3.5 turbo model is used for both text-to-Cypher and for\n",
    "answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = KuzuQAChain.from_llm(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then ask questions in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (:Taster {taster_name: 'Roger Voss'})-[:Tasted]->(wine:Wine)\n",
      "RETURN COUNT(wine)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(wine._ID)': 4}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How many wines has Roger Voss tasted?',\n",
       " 'result': 'Roger Voss has tasted 4 wines.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How many wines has Roger Voss tasted?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c:Customer)-[:Purchased]->(:Wine)<-[:Tasted]-(:Taster {taster_name: 'Roger Voss'})\n",
      "RETURN c.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c.name': 'Felicia Burke'}, {'c.name': 'Jill Wallace'}, {'c.name': 'Danielle Berry'}, {'c.name': 'Betty Johnson'}, {'c.name': 'William Hendricks'}, {'c.name': 'Colleen Campbell'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Give me the full name of the customers who purchased wine that was tasted by Roger Voss?',\n",
       " 'result': 'Felicia Burke, Jill Wallace, Danielle Berry, Betty Johnson, William Hendricks, Colleen Campbell purchased wine that was tasted by Roger Voss.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Give me the full name of the customers who purchased wine that was tasted by Roger Voss?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with open source LLMs\n",
    "You can use different LLMs, including open source ones, for text-to-Cypher and the answer generation stages. See the\n",
    "[LangChain docs](https://python.langchain.com/v0.2/docs/integrations/graphs/kuzu_db/#use-separate-llms-for-cypher-and-answer-generation)\n",
    "for such an example.\n",
    "\n",
    "Open source LLMs can be self-hosted and served on a local endpoint. In\n",
    "this example, we use a _much_ cheaper locally running `Mistral-7B-OpenOrca-GGUF` model from LMStudio\n",
    "for text-to-Cypher, followed by OpenAI's GPT-3.5 turbo model for answer generation. We are still able\n",
    "to call the `ChatOpenAI` class in both cases because LMStudio's local server mimics OpenAI's API endpoints.\n",
    "\n",
    "Note that cheap and small open source LLMs may not perform as well as the proprietary, general-purpose ones,\n",
    "so to obtain best performance on Cypher generation (as well as inference from Cypher quuery results),\n",
    "you may need to fine-tune a more powerful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = KuzuQAChain.from_llm(\n",
    "    qa_llm=ChatOpenAI(base_url=\"http://localhost:1234/v1\", temperature=0, api_key=\"not_needed\"),\n",
    "    cypher_llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    # qa_llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\")),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new KuzuQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (w:Wine)-[:IsFrom]->(c:Country)\n",
      "WHERE w.points >= 80 AND w.points <= 95\n",
      "WITH c, COUNT(w) AS wineCount\n",
      "RETURN c.country, wineCount\n",
      "ORDER BY wineCount DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c.country': 'US', 'wineCount': 8}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which country has the most wines that score between 80 and 95 points?',\n",
       " 'result': ' The United States has the most wines that score between 80 and 95 points.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Which country has the most wines that score between 80 and 95 points?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Feel free to experiment with other LLMs and see how they perform on your own data. As the natural\n",
    "language questions become more complex, it might result in incorrect Cypher generation, no matter\n",
    "how good the underlying LLM. In such cases, a query rewriting step may be required to provide better\n",
    "context to the cypher-generating LLM.\n",
    "\n",
    "This notebook is just the starting point of utilizing knowledge graphs for retrieval and QA tasks. You can\n",
    "look at more advanced pipelines that utilize agents, memory and routers via the LangChain and LlamaIndex frameworks,\n",
    "or simply roll out your own abstractions that fit your use case.\n",
    "\n",
    "Have fun using graphs, and `pip install kuzu`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindstone",
   "language": "python",
   "name": "mindstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
